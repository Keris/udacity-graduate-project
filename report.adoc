= æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçº³ç±³å­¦ä½
:author: æœ åˆ©å¼º
:email: dlq137@gmail.com
:revnumber: v0.1
:revdate: 01.04.2019
:revremark: First Draft
:sectnums:
:sectlinks:
:toc: left
:toclevels: 3
:source-highlighter: pygments
:pygments-style: xcode
:icons: font
:imagesdir: img
:media: prepress

== é¡¹ç›®èƒŒæ™¯

è¿‘å¹´æ¥ï¼Œ*æ·±åº¦å­¦ä¹ ï¼ˆDeep Leaningï¼ŒDLï¼‰* å¾—åˆ°äº†å¿«é€Ÿçš„å‘å±•ã€‚åœ¨2015å¹´åº•ï¼Œå¾®è½¯äºšæ´²ç ”ç©¶é™¢çš„ç ”ç©¶å‘˜æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºäºCNNçš„æ·±åº¦æ¨¡å‹ï¼Œç§°ä¹‹ä¸ºæ®‹å·®ç½‘ç»œï¼Œè¯¥æ®‹å·®ç½‘ç»œæ·±åº¦é«˜è¾¾152å±‚ï¼Œå–å¾—äº†å½“æ—¶å›¾åƒè¯†åˆ«æ¯”èµ›ä¸Šé¢æœ€å¥½çš„æˆç»©ã€‚
åœ¨2016å¹´åˆï¼ŒDeeopMindåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å¼€å‘çš„AlphaGoæ‰“è´¥äº†å›´æ£‹ä¸–ç•Œå† å†›æä¸–çŸ³ã€‚æ·±åº¦å­¦ä¹ çš„æˆåŠŸä¸»è¦å½’åŠŸäºä¸‰å¤§å› ç´ ï¼š*å¤§æ•°æ®ã€å¤§æ¨¡å‹å’Œå¤§è®¡ç®—*ã€‚
 ç°é˜¶æ®µå¯ä»¥åˆ©ç”¨çš„æ•°æ®ç‰¹åˆ«æ˜¯æ ‡æ³¨æ•°æ®éå¸¸å¤šï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦åˆ°ä»¥å‰æ²¡æ³•å­¦ä¹ çš„ä¸œè¥¿ã€‚å¦å¤–ï¼ŒæŠ€æœ¯ä¸Šçš„å‘å±•ä½¿å¾—è®­ç»ƒå¤§æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œå¦‚ä¸Šåƒå±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚æœ€åï¼Œå¯è·å–çš„è®¡ç®—èµ„æºè¶Šæ¥è¶Šä¸°å¯Œï¼Œä»CPUåˆ°GPUåˆ°FPGAï¼Œè¿™å±äºå¤§è®¡ç®—ã€‚

ä»¥ä¸ŠæåŠçš„ä¸‰å¤§è¦ç´ æˆå°±äº†æ·±åº¦å­¦ä¹ ï¼Œä½†åŒæ—¶ä¹Ÿä¸ºæ·±åº¦å­¦ä¹ çš„è¿›ä¸€æ­¥å‘å±•å’Œæ™®å¸¦æ¥äº†ä¸€äº›åˆ¶çº¦å› ç´ ã€‚
é¦–å…ˆï¼Œæ ‡æ³¨æ•°æ®ä»£ä»·æ˜‚è´µï¼Œä»æ— æ ‡æ³¨çš„æ•°æ®é‡Œå­¦ä¹ æˆä¸ºæ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªå‰æ²¿ï¼Œå·²ç»æœ‰è¿™æ–¹é¢çš„ç ”ç©¶å·¥ä½œï¼ŒåŒ…æ‹¬ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œå’Œå¯¹å¶å­¦ä¹ ã€‚
äºŒæ˜¯å¦‚ä½•å¾—åˆ°ä¸€äº›æ¯”è¾ƒå°çš„æ¨¡å‹ä½¿å¾—æ·±åº¦å­¦ä¹ æŠ€æœ¯èƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡å’Œå„ç§åœºæ‰€é‡Œé¢å¾—åˆ°æ›´å¹¿æ³•çš„åº”ç”¨ã€‚
ä¸‰æ˜¯å¦‚ä½•è®¾è®¡æ›´å¿«æ›´é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚å››æ˜¯å¦‚ä½•æŠŠæ•°æ®å’ŒçŸ¥è¯†ç»“åˆèµ·æ¥ã€‚äº”æ˜¯å¦‚ä½•æŠŠæ·±åº¦å­¦ä¹ çš„æˆåŠŸä»ä¸€äº›é™æ€çš„ä»»åŠ¡æ‰©å±•åˆ°å¤æ‚çš„åŠ¨æ€å†³ç­–ä»»åŠ¡ä¸Šå»ã€‚

å‰è¿°æåˆ°çš„ç¬¬äº”ç‚¹ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learning, DRLï¼‰å°±æ˜¯é’ˆå¯¹å…¶åšçš„ä¸€ä¸ªå¾ˆå¥½çš„å°è¯•ã€‚
2015å¹´ï¼ŒDeepMindçš„Volodymyr Mnihç­‰ç ”ç©¶å‘˜åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šå‘è¡¨è®ºæ–‡ _Human-level control through deep reinforcement learning_ï¼Œ
è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æŠ€æœ¯å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ€æƒ³çš„æ¨¡å‹Deep Q-Network(DQN)ï¼Œåœ¨Atariæ¸¸æˆå¹³å°ä¸Šå±•ç¤ºå‡ºè¶…è¶Šäººç±»æ°´å¹³çš„è¡¨ç°ã€‚
è‡ªæ­¤ä»¥åï¼Œç»“åˆDLä¸RLçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learning, DRLï¼‰è¿…é€Ÿæˆä¸ºäººå·¥æ™ºèƒ½ç•Œçš„ç„¦ç‚¹ã€‚

å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼Œ RLï¼‰æ˜¯ä¸å†³ç­–æ‰§è¡Œï¼ˆdecision makingï¼‰å’Œç”µæœºæ§åˆ¶ï¼ˆmotor controlï¼‰æœ‰å…³çš„æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚å®ƒç ”ç©¶agentå¦‚ä½•åœ¨å¤æ‚ã€ä¸ç¡®å®šçš„ç¯å¢ƒä¸­å­¦ä¹ å¦‚ä½•å®ç°ç›®æ ‡ã€‚

å¼ºåŒ–å­¦ä¹ èƒ½åšä»€ä¹ˆä»¥åŠè¡¨ç°ï¼š

* *RLéå¸¸é€šç”¨ï¼Œæ¶µç›–äº†æ¶‰åŠåšå‡ºä¸€ç³»åˆ—å†³ç­–çš„æ‰€æœ‰é—®é¢˜ã€‚* ä¾‹å¦‚ï¼Œæ§åˆ¶æœºå™¨äººçš„ç”µæœºï¼Œä½¿å…¶èƒ½å¤Ÿè·‘å’Œè·³ï¼Œåšå‡ºå•†ä¸šå†³ç­–ï¼Œå¦‚å®šä»·å’Œåº“å­˜ç®¡ç†ï¼Œæˆ–ç©è§†é¢‘æ¸¸æˆå’Œæ£‹ç‰Œæ¸¸æˆã€‚RLç”šè‡³å¯ä»¥åº”ç”¨äºå…·æœ‰é¡ºåºæˆ–ç»“æ„åŒ–è¾“å‡ºçš„ç›‘ç£å­¦ä¹ é—®é¢˜ã€‚
* *RLç®—æ³•å·²ç»å¼€å§‹åœ¨è®¸å¤šå›°éš¾ç¯å¢ƒä¸­å–å¾—è‰¯å¥½æ•ˆæœã€‚*  æ­£å¦‚å‰é¢æåˆ°çš„ï¼Œæ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆä½¿DRLæˆä¸ºäº†äººå·¥æ™ºèƒ½çš„ç„¦ç‚¹ï¼Œæœ‰å¾ˆå¤šäº®çœ¼è¡¨ç°ï¼šåœ¨è§†é¢‘æ¸¸æˆï¼ˆDotaï¼‰ã€æ£‹ç±»æ¸¸æˆï¼ˆå›´æ£‹ï¼‰ä¸Šæ‰“è´¥äººç±»é¡¶å°–é«˜æ‰‹ï¼›æ§åˆ¶å¤æ‚çš„æœºæ¢°è¿›è¡Œæ“ä½œï¼›è°ƒé…ç½‘ç»œèµ„æºï¼›ä¸ºæ•°æ®ä¸­å¿ƒå¤§å¹…èŠ‚èƒ½ï¼›ç”šè‡³å¯¹æœºå™¨å­¦ä¹ ç®—æ³•è‡ªåŠ¨è°ƒå‚ã€‚

ç„¶è€Œï¼ŒRLçš„ç ”ç©¶ä¹Ÿå­˜åœ¨åˆ¶çº¦å› ç´ ï¼š

* *éœ€è¦å»ºç«‹æ›´å¥½çš„åŸºå‡†ã€‚* åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼ŒImageNetè¿™æ ·çš„å¤§å‹æ ‡è®°æ•°æ®é›†é©±åŠ¨äº†å…¶å‘å±•ã€‚åœ¨RLä¸­ï¼Œæœ€æ¥è¿‘çš„ç­‰ä»·ç‰©æ˜¯å¤§é‡ä¸”å¤šæ ·åŒ–çš„ç¯å¢ƒé›†åˆã€‚ ä½†æ˜¯ï¼ŒRLç¯å¢ƒçš„ç°æœ‰å¼€æºé›†åˆæ²¡æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼Œå¹¶ä¸”å®ƒä»¬é€šå¸¸éš¾ä»¥è®¾ç½®å’Œä½¿ç”¨ã€‚åœ¨è¿™æ–¹é¢ï¼ŒOpenAIåšå‡ºäº†çªå‡ºçš„è´¡çŒ®ã€‚
* *ä½¿ç”¨çš„ç¯å¢ƒç¼ºä¹æ ‡å‡†åŒ–ã€‚* é—®é¢˜å®šä¹‰ä¸­çš„ç»†å¾®å·®åˆ«ï¼Œä¾‹å¦‚å¥–åŠ±å‡½æ•°æˆ–åŠ¨ä½œé›†ï¼Œå¯ä»¥æå¤§åœ°æ”¹å˜ä»»åŠ¡çš„éš¾åº¦ã€‚è¿™ä¸ªé—®é¢˜å¯¼è‡´DRLçš„å¯å¤ç°æ€§å±æœºã€‚

è°ƒæŸ¥å‘ç°ï¼ŒRLå·²ç»åœ¨å„ä¸ªé¢†åŸŸè¢«å¹¿æ³›ä½¿ç”¨ï¼š

. æ§åˆ¶é¢†åŸŸã€‚æ˜¯RLæ€æƒ³çš„å‘æºåœ°ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯RLæŠ€æœ¯åº”ç”¨æœ€æˆç†Ÿçš„é¢†åŸŸã€‚
. è‡ªåŠ¨é©¾é©¶é¢†åŸŸã€‚ä»80å¹´ä»£çš„ALVINNã€TORCSåˆ°å¦‚ä»Šçš„CARLAã€‚
. NLPé¢†åŸŸã€‚å¯¹è¯ç³»ç»Ÿã€æœºå™¨å†™ä½œç­‰ã€‚
. æ¨èç³»ç»Ÿä¸æ£€ç´¢ç³»ç»Ÿé¢†åŸŸã€‚
. é‡‘èé¢†åŸŸã€‚
. è¿›è¡Œæ•°æ®é€‰æ‹©ã€‚
. é€šè®¯ã€ç”Ÿäº§è°ƒåº¦ã€è§„åˆ’å’Œèµ„æºè®¿é—®æ§åˆ¶ç­‰è¿ç­¹é¢†åŸŸã€‚

**å››è½´é£è¡Œå™¨**åœ¨ä¸ªäººå’Œä¸“ä¸šåº”ç”¨é¢†åŸŸéƒ½å˜å¾—è¶Šæ¥è¶Šçƒ­é—¨ã€‚å®ƒæ˜“äºæ“æ§ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œä»æœ€åä¸€å…¬é‡ŒæŠ•é€’åˆ°ç”µå½±æ‘„å½±ï¼Œä»æ‚æŠ€è¡¨æ¼”åˆ°æœæ•‘ï¼Œæ— æ‰€ä¸åŒ…ã€‚

image::parrot-ar.png[æ— äººæœº]

å››è½´é£è¡Œå™¨æ§åˆ¶æ˜¯å¼ºåŒ–å­¦ä¹ åœ¨æ§åˆ¶é¢†åŸŸçš„ä¸€ä¸ªå…¸å‹åº”ç”¨ï¼Œåœ¨è¯¥é¡¹ç›®ä¸­éœ€è¦è®¾è®¡ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ¥æ§åˆ¶å‡ ä¸ªå››è½´é£è¡Œå™¨çš„é£è¡Œä»»åŠ¡ï¼ŒåŒ…æ‹¬èµ·é£ã€æ‚¬åœå’Œç€é™†ã€‚ç›¸æ¯”è€Œè¨€ï¼ŒDLä¸­å­¦ä¹ å™¨ï¼ˆlearnerï¼‰éƒ½æ˜¯å­¦ä¼šæ€ä¹ˆåšï¼Œè€Œåœ¨RLä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨ä¸ç¯å¢ƒä¸æ–­äº’åŠ¨å’Œæ¢ç´¢çš„è¿‡ç¨‹ä¸­æ ¹æ®å¥–åŠ±æˆ–æƒ©ç½šé€‰æ‹©åŠ¨ä½œä¸æ–­å­¦ä¹ ï¼Œè¿™ååˆ†æœ‰è¶£ï¼Œå¹¶ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

== é—®é¢˜æè¿°

é¡¹ç›®çš„ç›®çš„ä¸ºè®­ç»ƒä¸€ä¸ªå››è½´é£è¡Œå™¨å­¦ä¼šå¦‚ä½•é£è¡Œã€‚å…·ä½“ä¸ºè®¾è®¡ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ¥æ§åˆ¶å‡ ä¸ªå››è½´é£è¡Œå™¨çš„é£è¡Œä»»åŠ¡ï¼ŒåŒ…æ‹¬èµ·é£ã€æ‚¬åœå’Œç€é™†ã€‚åœ¨æ­£å¼å¼€å§‹ä¹‹å‰ï¼Œéœ€è¦å®‰è£…ROSä»¥åŠä¸‹è½½å¯¹åº”çš„æ¨¡æ‹Ÿå™¨ï¼Œç„¶åè¿è¡ŒROSå’Œæ¨¡æ‹Ÿå™¨å¹¶ç¡®ä¿ä¸¤è€…èƒ½å¤Ÿæ­£å¸¸è¿æ¥ï¼Œè¯¦ç»†çš„æ­¥éª¤å‚è§ https://github.com/udacity/RL-Quadcopter[README]ã€‚ è¿™ä¸€åˆ‡å°±ç»ªåï¼Œæœ€åæ‰€è¦åšçš„å°±æ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡æ­£ç¡®å®šä¹‰ *Task* å’Œ *Agent* ï¼Œå…±æ¶‰åŠå››ä¸ªä»»åŠ¡:

* ä»»åŠ¡1ï¼š èµ·é£
+
è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“æˆåŠŸåœ°ä»åœ°é¢èµ·é£ä¸”åˆ°è¾¾æŸä¸ªé˜ˆå€¼é«˜åº¦ï¼ˆå¦‚åœ°ä¸Š10ä¸ªå•ä½ï¼‰ã€‚
* ä»»åŠ¡2ï¼š æ‚¬åœ
+
è®­ç»ƒæ™ºèƒ½ä½“ä½¿å…¶èµ·é£ä¸”æ‚¬åœåœ¨è®¾å®šçš„é«˜åº¦ä¸€æ®µæ—¶é—´ã€‚
* ä»»åŠ¡3ï¼š ç€é™†
+
è®­ç»ƒæ™ºèƒ½ä½“å­¦ä¼šä»åœ°ä¸ŠæŸä¸ªä½ç½®æ¸©å’Œåœ°ç€é™†ã€‚
* ä»»åŠ¡4ï¼š å…¨å¥—åŠ¨ä½œ
+
å®ç°ä¸€ä¸ªç«¯åˆ°ç«¯çš„ä»»åŠ¡ï¼Œå³èµ·é£ï¼ŒåŸåœ°æ‚¬åœæŸä¸ªæ—¶é•¿ï¼Œç„¶åç€é™†ã€‚

== æ•°æ®é›†å’Œè¾“å…¥

åœ¨è¿™é‡Œï¼Œæ•´ä¸ªç³»ç»ŸåŒ…æ‹¬ROSï¼Œç¯å¢ƒå’Œæ™ºèƒ½ä½“ã€‚ç¯å¢ƒå’ŒåŠ¨ä½œéƒ½æ˜¯è¿ç»­çš„ï¼Œæ‰€ä»¥éƒ½è¢«å®šä¹‰ä¸º https://github.com/openai/gym/blob/master/gym/spaces/box.py[Box] ç©ºé—´ã€‚ç¯å¢ƒä¸ºä¸€ä¸ª `cube, cube_size x cube_size x cube_size`ï¼Œ æ¯ä¸ªçŠ¶æ€è¡¨ç¤ºä¸ºä¸€ä¸ª7ç»´å‘é‡ `<position_x, .._y, .._z, orientation_x, .._y, .._z, .._w>`ï¼Œ å‰ä¸‰ä¸ªåˆ†é‡è¡¨ç¤ºä½ç½®ï¼ˆposeï¼‰ï¼Œåå››ä¸ªåˆ†é‡ç»„æˆä¸€ä¸ªå››å…ƒæ•°è¡¨ç¤ºæ–¹å‘ï¼ˆorientation)ã€‚æ¯ä¸ªåŠ¨ä½œè¡¨ç¤ºä¸ºä¸€ä¸ª6ç»´å‘é‡ `<force_x, .._y, .._z, torque_x, .._y, .._z>`ï¼Œ å‰ä¸‰ä¸ªåˆ†é‡ç»„æˆåŠ›ï¼ˆforceï¼‰çš„è¡¨ç¤ºï¼Œåä¸‰ä¸ªåˆ†é‡ç»„æˆæ‰­çŸ©ï¼ˆtorqueï¼‰çš„è¡¨ç¤ºã€‚

é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ï¼ŒçŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´å¯ä»¥è¿›è¡Œä¸€å®šçš„ä¼˜åŒ–ã€‚æ¯”å¦‚ï¼Œåœ¨èµ·é£ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä¸å…³æ³¨é£è¡Œå™¨çš„æ–¹å‘ï¼Œåªéœ€çŸ¥é“é£è¡Œå™¨çš„ä½ç½®ï¼ˆ*çŠ¶æ€å‘é‡çš„å‰3ä¸ªåˆ†é‡*ï¼‰ã€‚åŒæ ·ï¼Œåªéœ€è¦åº”ç”¨çº¿æ€§æ¨åŠ›ï¼Œä¸éœ€è¦åº”ç”¨æ‰­çŸ©ï¼ˆ*è¿™æ ·è¿˜ä¼šé˜²æ­¢å‡ºç°ä¸å¿…è¦çš„æ—‹è½¬å’Œæ­ªæ–œ*ï¼‰ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦é£è¡Œå™¨ç¦»åœ°ä¸”è¾¾åˆ°æŸä¸ªé«˜åº¦ï¼Œå¥–åŠ±å¯ä»¥æ ¹æ®é£è¡Œå™¨å½“å‰é«˜åº¦ä¸ç›®æ ‡é«˜åº¦çš„è·ç¦»æ¥åº¦é‡ï¼Œå½“å…¶è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡é«˜åº¦æ—¶è¿›è¡Œé¢å¤–å¥–åŠ±ï¼Œå¦‚æœåœ¨è®¾å®šæ—¶é—´éƒ½æ²¡æœ‰å®Œæˆï¼Œåˆ™è¿›è¡Œé¢å¤–æƒ©ç½šã€‚

æ­¤å¤–ï¼Œæ¯ç§ä»»åŠ¡ä¸‹é£è¡Œå™¨çš„èµ·å§‹çŠ¶æ€ä¸åŒã€‚æ¯”å¦‚åœ¨èµ·é£ï¼ˆTakeoffï¼‰ä»»åŠ¡ä¸­ï¼Œé£è¡Œå™¨ä¸€å¼€å§‹å¤„äºæŸä¸ªéšæœºé«˜åº¦ï¼ˆå¦‚å¯ä»¥é‡‡æ ·è‡ªå‡å€¼ä¸º0.5ï¼Œæ ‡å‡†å·®ä¸º0.1çš„æ­£æ€åˆ†å¸ƒï¼‰ã€‚è€Œåœ¨ç€é™†ï¼ˆLandingï¼‰ä»»åŠ¡ä¸­ï¼Œé£è¡Œå™¨ä¸€å¼€å§‹å¤„äºç¦»åœ°æŸä¸ªé«˜åº¦ï¼ˆå¦‚ç¦»åœ°10ä¸ªå•ä½ï¼‰ï¼Œå¹¶ä¸”å…·æœ‰ä¸€å®šçš„çº¿æ€§æ¨åŠ›ã€‚


[#Solution-Description]
== è§£å†³æ–¹æ¡ˆæè¿°

é¡¹ç›®æ¶‰åŠçš„ç¯å¢ƒå’ŒçŠ¶æ€éƒ½æ˜¯è¿ç»­ç©ºé—´ï¼Œè¿™æ„å‘³ç€åªèƒ½é€‰æ‹©é€‚åˆè¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„ç®—æ³•æˆ–è€…å°†è¿ç»­ç©ºé—´ç¦»æ•£åŒ–ã€‚DQNçš„é€‚ç”¨èŒƒå›´æ˜¯ä½ç»´ã€ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œåœ¨è¿ç»­ç©ºé—´ä¸èƒ½é€‚ç”¨ï¼š

. å¦‚æœé‡‡ç”¨æŠŠè¿ç»­åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–ï¼ŒåŠ¨ä½œç©ºé—´åˆ™ä¼šè¿‡å¤§ï¼Œæéš¾æ”¶æ•›ã€‚
. å³ä½¿æœ‰äº›DQNçš„å˜ç§å¦‚VAEèƒ½æä¾›è¿ç»­åŠ¨ä½œçš„æ–¹æ¡ˆï¼Œä½†DQNåªèƒ½ç»™å‡ºä¸€ä¸ªç¡®å®šæ€§çš„åŠ¨ä½œï¼Œæ— æ³•ç»™å‡ºæ¦‚ç‡å€¼ã€‚

ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼ŒDQNæ˜¯å€¼åŸºäºçš„æ–¹æ³•ï¼Œæœ€ç»ˆè¿˜æ˜¯æ±‚è§£ç­–ç•¥ã€‚ä½•ä¸ä¸€å¼€å§‹å°±æ±‚è§£ç­–ç•¥ï¼Œè¿™å°±æ˜¯ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰æ–¹æ³•ã€‚åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œè®¾å®šå‚æ•°åŒ–ç­–ç•¥ï¼Œç„¶åè®¡ç®—å¾—åˆ°åŠ¨ä½œä¸Šçš„ç­–ç•¥æ¢¯åº¦ï¼Œæ²¿ç€æ¢¯åº¦æ–¹å‘ï¼Œé€æ­¥è°ƒæ•´åŠ¨ä½œï¼Œé€æ¸å¾—åˆ°æœ€ä¼˜ç­–ç•¥ã€‚ç­–ç•¥æ¢¯åº¦æ–¹æ³•åŒ…æ‹¬éšæœºç­–ç•¥æ¢¯åº¦ï¼ˆSPGï¼‰å’Œç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDPGï¼‰ã€‚

https://arxiv.org/pdf/1509.02971.pdf[DDPG] æ˜¯ç»“åˆäº†DQNå’ŒDPGï¼ŒæŠŠDRLæ¨å‘äº† *è¿ç»­åŠ¨ä½œç©ºé—´æ§åˆ¶*ã€‚ DDPGå®é™…ä¸ºä¸€ç§è¡ŒåŠ¨è€…-è¯„ä»·è€…ï¼ˆactor-criticï¼‰æ–¹æ³•ï¼Œå…¶æ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

image::actor-critic-frame.png[Actor-Critic æ–¹æ³•]

Actorç½‘ç»œçš„è¾“å…¥æ˜¯stateï¼Œè¾“å‡ºæ˜¯actionï¼Œä½¿ç”¨DNNè¿›è¡Œå‡½æ•°æ‹Ÿåˆï¼ŒNNè¾“å‡ºå±‚ä½¿ç”¨tanhæˆ–sigmoidæ¿€åŠ±ã€‚Criticç½‘ç»œçš„è¾“å…¥æ˜¯stateå’Œactionï¼Œè¾“å‡ºä¸ºQå€¼ã€‚æ­¤å¤–ï¼ŒDDPGä¸­å€Ÿé‰´äº†DQNä¸­çš„ç»éªŒå›æ”¾å’Œtargetç½‘ç»œï¼Œå¹¶ä¸”Actorå’ŒCriticçš„targetç½‘ç»œå‡ä»¥å°æ­¥é•¿æ»åæ›´æ–°ï¼Œç›®çš„æ˜¯è®©æ¨¡å‹è®­ç»ƒçš„æ›´ç¨³å®šã€‚æœ€åï¼Œé€šè¿‡åœ¨actionçš„åŸºç¡€ä¸Šå¢åŠ éšæœºå™ªå£°è®©æ™ºèƒ½ä½“æœ‰æœºä¼šæ¢ç´¢ç¯å¢ƒã€‚

åœ¨è¯¥é¡¹ç›®ä¸­ï¼Œé€‰æ‹©å®ç°DDPGæ–¹æ³•ä»¥å®Œæˆé£è¡Œå™¨çš„æ§åˆ¶ä»»åŠ¡ã€‚

== åŸºå‡†æ¨¡å‹

è¿™é‡Œé€‰æ‹©çº¿æ€§ç­–ç•¥ä½œä¸ºåŸºå‡†æ¨¡å‹ï¼Œè®¾ç½®å¦‚ä¸‹ï¼š

1. éšæœºåˆå§‹åŒ–ç­–ç•¥å‚æ•°
+
[source,python]
----
# Policy parameters
self.w = np.random.normal(
    size=(self.state_size, self.action_size),  # weights for simple linear policy: state_space x action_space
    scale=(self.action_range / (2 * self.state_size)).reshape(1, -1))  # start producing actions in a decent range
----
2. ä½¿ç”¨çº¿æ€§ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
+
[source,python]
----
def act(self, state):
   # Choose action based on given state and policy
   action = np.dot(state, self.w)  # simple linear policy
   return action
----
3. æ›´æ–°ç­–ç•¥å‚æ•°
+
[source,python]
----
self.w = self.w + self.noise_scale * np.random.normal(size=self.w.shape)  # equal noise in all directions
----

åŸºå‡†æ¨¡å‹çš„å­˜åœ¨æ˜¯ä¸ºäº†è¡¡é‡è¦å®ç°çš„DDPGæ–¹æ³•çš„æ§åˆ¶æ•ˆæœï¼Œè¿™å¯ä»¥é€šè¿‡ä¸‹ä¸ªéƒ¨åˆ†æŒ‡å®šçš„<<Metrics>>è¿›è¡Œå¯¹æ¯”ã€‚

[#Metrics]
== è¯„ä¼°æŒ‡æ ‡

åœ¨å¯¹å¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆè¿›è¡Œè¿­ä»£æ—¶ï¼Œéœ€è¦äº†è§£æ™ºèƒ½ä½“çš„è¡¨ç°ï¼Œè¿™å°±éœ€è¦å»ºç«‹è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨è¯¥é¡¹ç›®ä¸­ï¼Œä¸€ä¸ªç®€å•æœ‰æ•ˆçš„è¯„ä»·æŒ‡æ ‡æ˜¯æ™ºèƒ½ä½“åœ¨æ¯ä¸ªé˜¶æ®µè·å–çš„æ€»å¥–åŠ±æˆ–æœ€è¿‘è‹¥å¹²ä¸ªï¼ˆæ¯”å¦‚10ä¸ªï¼‰é˜¶æ®µçš„å¹³å‡å¥–åŠ±å˜åŒ–ï¼Œè¿™å¯ä»¥é€šè¿‡ä¿å­˜é˜¶æ®µç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶ï¼Œç»˜åˆ¶episode rewardsè¿›è¡Œå¯¹æ¯”ã€‚

== é¡¹ç›®è®¾è®¡

åœ¨<<Solution-Description>>éƒ¨åˆ†å·²ç»ç¡®å®šäº†è¦ä½¿ç”¨çš„ç®—æ³•ä¸ºDDPGã€‚æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼š

1. å®šä¹‰ä»»åŠ¡ï¼ˆä» `BaseTask` ç»§æ‰¿å®šä¹‰å…·ä½“ä»»åŠ¡ç±»ï¼‰

   * åœ¨ `+++__init__()+++` æ„é€ å‡½æ•°ä¸­å®Œæˆè§‚å¯Ÿç©ºé—´ï¼ˆobservation space)å’ŒåŠ¨ä½œç©ºé—´ï¼ˆaction spaceï¼‰çš„å®šä¹‰ï¼Œè®¾ç½®ä¸€äº›ä»»åŠ¡ç›¸å…³çš„å‚æ•°ï¼Œå¦‚ç›®æ ‡é«˜åº¦ï¼Œæœ€å¤§ç”¨æ—¶ç­‰ã€‚
   * å®ç° `reset()` æ–¹æ³•å’Œ `update()` æ–¹æ³•ï¼Œå‰è€…åœ¨ä¸€å¼€å§‹æˆ–è€…é˜¶æ®µç»“æŸæ—¶è°ƒç”¨ï¼Œç”¨äºé‡ç½®é£è¡Œå™¨åˆ°èµ·å§‹çŠ¶æ€ã€‚åè€…æ ¹æ®å½“å‰çš„çŠ¶æ€å¯¹æ™ºèƒ½ä½“è¿›è¡Œå¥–æƒ©ï¼Œåˆ¤æ–­é˜¶æ®µæ˜¯å¦ç»“æŸï¼Œæœ€åè¿”å›ä¸€ä¸ªåŠ¨ä½œã€‚

2. å®šä¹‰æ™ºèƒ½ä½“ï¼ˆä» `BaseAgent` ç»§æ‰¿å®šä¹‰å…·ä½“Agentç±»ï¼‰

    * åœ¨ `pass:[__init__()]` æ„é€ å‡½æ•°ä¸­ï¼š
      * å®šä¹‰æ™ºèƒ½ä½“çš„ä»»åŠ¡ï¼ŒçŠ¶æ€å’Œç©ºé—´å¤§å°ï¼›
      * å®šä¹‰Actorå’ŒCriticç½‘ç»œï¼ˆåŒ…æ‹¬æœ¬åœ°å’Œtargetï¼‰å¹¶åˆå§‹åŒ–ï¼›
      * è®¾ç½®ç»éªŒå›æ”¾å’Œå™ªå£°è¿‡ç¨‹ï¼›
      * è®¾ç½®å…¶ä»–å‚æ•°ï¼Œå¦‚gammaï¼ˆæŠ˜æ‰£ç‡ï¼‰ï¼Œtauï¼ˆè½¯æ›´æ–°å› å­ï¼‰ä»¥åŠå›æŠ¥ç»Ÿè®¡ä¿å­˜æ–‡ä»¶åç­‰ã€‚
    * å®ç° `step()` æ–¹æ³•ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®å½“å‰çš„çŠ¶æ€sï¼Œå›æŠ¥ä»¥åŠå®Œæˆæ ‡ç­¾é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œä½¿ç”¨ç»éªŒå›æ”¾æŠ€æœ¯è®­ç»ƒæ¨¡å‹ä»¥åŠä¿å­˜å›æŠ¥ç»Ÿè®¡ä¿¡æ¯ã€‚

3. è¿è¡Œä»»åŠ¡

    * è¿è¡ŒROSå’Œæ¨¡æ‹Ÿå™¨
    * å½•å±æœ€ç»ˆçš„è®­ç»ƒæ•ˆæœ

4. ç»˜åˆ¶é˜¶æ®µå›æŠ¥å¹¶è¿›è¡Œæ€§èƒ½åˆ†æ

åœ¨è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç»˜åˆ¶é˜¶æ®µå›æŠ¥ä¿¡æ¯ï¼š

[source, python]
----
import pandas as pd

df_stats = pd.read_csv(<csv_filename>)
df_stats[['total_reward']].plot(title="Episode Rewards")
----

image::episode-rewards.png[Episode Rewards]

=== Takeoff ä»»åŠ¡

*ç›®æ ‡*ï¼šè®­ç»ƒæ™ºèƒ½ä½“æˆåŠŸåœ°ä»åœ°é¢èµ·é£ï¼Œç„¶åè¾¾åˆ°æŸä¸ªé«˜åº¦ã€‚

Episode Rewards Plot:

image::takeoff/takeoff-episode-rewards.png[episode-rewards]

*Q:* What algorithm did you use? Briefly discuss why you chose it for this task.
  
*A:* I used an algorithm called DDPG, which is short for Deep Deterministic Policy Gradients and comes from the paper Continuous control with deep reinforcement learning. DDPG is a model-free policy based learning algorithm in which the agent learns directly from the unprocessed observation space without knowing the domain dynamic information, making it suited to solve control problems in continuous action space. It also employs actor-critic method in which actor model maps states into actions while critic model maps state-action pair into Q value. The visualization of actor and critic models is as following:

Actor:

image::takeoff/actor.png[actor model]

Critic:

image::takeoff/critic.png[critic model]

*Q:* Using the episode rewards plot, discuss how the agent learned over time.

* Was it an easy task to learn or hard?
* Was there a gradual learning curve, or an aha moment?
* How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)

*A*: It was an easy task to learn, from episode rewards plot we can see after around only 25 episodes the agent manages to learn how to take off.
In this training case, there was not a gradual learning curve but an aha moment, which is around the 25th episode, before that moment the agent moves around but its height doesn't increase, leading to unchanged reward, after that moment the agent manages to learn how to lift from ground.
The mean rewards over the last 10 episodes is around -600.

=== Hover ä»»åŠ¡

*ç›®æ ‡ï¼š* è®­ç»ƒæ™ºèƒ½ä½“èµ·é£å¹¶ä¸”æ‚¬åœåœ¨æŸä¸ªç‚¹ï¼ˆå¦‚åœ°ä¸Š10ä¸ªå•ä½é«˜åº¦ï¼‰ã€‚

Episode Rewards Plot:

image::hover/episode-rewards.png[episode-rewards]

*Q:* Did you change the state representation or reward function? If so, please explain below what worked best for you, and why you chose that scheme. Include short code snippet(s) if needed.

*A:* Yes, I changed the state representation, reward function was also changed. The new state representation is as following:

[source,python]
----
if self.last_pose is None:
            dist_last_pose = np.array([0., 0., 0.])
else:
    dist_last_pose = np.array([
        abs(pose.position.x - self.last_pose.position.x),
        abs(pose.position.y - self.last_pose.position.y),
        abs(pose.position.z - self.last_pose.position.z)
    ])
dist_target_z = abs(pose.position.z - self.target_z)

state = np.concatenate([
    np.array([pose.position.x, pose.position.y, pose.position.z]),
    dist_last_pose,
    np.array([dist_target_z])
])
----

As you can see, I included the difference between current pose and last pose, and the difference between current pose and target in z direction. These two components serves to make the agent sense whether leave target or current pose too far, thus making a well hovering.

As for reward function, the new one is as following:

[source,python]
----
reward = (10.0 - dist_target_z) * 0.8
if pose.position.z >= self.target_z:
    reward += 2.0  # give a small bonus
    if dist_target_z <= 2.0:
        reward += 5.0  # bonus reward, agent starts to hover
        if linear_acc:
            reward -= 0.1 * linear_acc
        if self.start_hover is None:
            self.start_hover = timestamp
        elif timestamp - self.start_hover >= 0.5:  # give reward if hover for some time
            reward += 2.0
    elif dist_target_z > 5.0:  # agent leaves target too far
        if self.count % 20 == 0:
            print("last duration: {}".format(timestamp))
        if self.start_hover is not None:
            print("last duration: {:.4f}".format(timestamp - self.start_hover))
        done = True
    else:
        self.start_hover = None
if timestamp > self.max_duration:  # agent has run out of time
    reward -= 5.0
    done = True
----

The reward function seems a litte complex. The base reward function is just like the one in takeoff task, when the agent leaves the target too far, it gets a smaller reward. Then if it cross over the target, it gets a small reward and if it stays in a some range between target, it gets a slight big reward. At the same time, the agent gets a penalty if it has a big linear acceration in z direction when hovering.

*å®ç°notes*

*Q:* Discuss your implementation below briefly, using the following questions as a guide:

* What algorithm(s) did you try? What worked best for you?
* What was your final choice of hyperparameters (such as  ğ›¼ ,  ğ›¾ ,  ğœ– , etc.)?
* What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.

*A:* Again I used DDPG algorithm, it's same with that used in takeoff task. Because the state representation was changed, so the states input layer of actor and critic models has different input shape as following:

Actor:

image::hover/actor.png[actor model]

Critic:

image::hover/critic.png[critic model]

* The hyperparameters  ğ›¼ ,  ğ›¾  used in this task is 0.001 and 0.99. A noise produced by Ornstein-Uhlenbeck process is added to action, where  ğœ‡ =0.0,  ğœƒ =0.15 and  ğœ =0.3.
* For neural network architecture please see beforemetioned actor and critic models diagram, the activation function is relu.

=== Landing ä»»åŠ¡

*ç›®æ ‡*ï¼šè®©æ™ºèƒ½ä½“æ¸©å’Œåœ°ç€åœ°ã€‚

Episode Rewards Plot:

image::landing/episode-rewards.png[episode-rewards]

*åˆå§‹æ¡ä»¶ï¼ŒçŠ¶æ€å’Œå›æŠ¥*

*Q:* How did you change the initial condition (starting state), state representation and/or reward function? Please explain below what worked best for you, and why you chose that scheme. Were you able to build in a reward mechanism for landing gently?

*A:* Because this time we need to make the quadcopter land, so the initial condition is the quadcopter is in the air as following:

[source,python]
----
def reset(self):
    ...
    return Pose(
                    position=Point(0.0, 0.0, 10.0),  # Land from height of 10 units above ground
                    orientation=Quaternion(0.0, 0.0, 0.0, 0.0),
                ), Twist(
                    linear=Vector3(0.0, 0.0, 0.0),
                    angular=Vector3(0.0, 0.0, 0.0)
                )
----

The state representation here is same with which used in hover task. The reward function (core part) is as following:

[source,python]
----
reward = (10.0 - dist_target_z) * 0.8
if pose.position.z <= 0.5:
    reward += 5.0  # give a small bonus
    if linear_acc:
        reward -= 0.1 * linear_acc
    done = True
elif timestamp > self.max_duration:  # agent has run out of time
    reward -= 5.0
    done = True
----

As before, the base part of reward function states that when the agent has a smaller difference with target, it gets a higher reward. When the agent falls in some small range between target, it gets a bonus, at the same time it gets penalty because of linear acceleration in z direction.

*å®ç°notes*

*Q:* Discuss your implementation below briefly, using the same questions as before to guide you.

*A:* The agent used here is totally same with the one used in hover task.

=== Reflections

*Q:* Briefly summarize your experience working on this project. You can use the following prompts for ideas.

What was the hardest part of the project? (e.g. getting started, running ROS, plotting, specific task, etc.)
How did you approach each task and choose an appropriate algorithm/implementation for it?
Did you find anything interesting in how the quadcopter or your agent behaved?

*A:* 

* The hardest part of the project for me was composing a well working state representation and reward function.
* All tasks are of continuous controlling, so a working agent implementation is needed. DDPG is a model-free policy based learning algorithm in which the agent learns directly from the unprocessed observation space without knowing the domain dynamic information, making it suited to solve control problems in continuous action space. So I choosed DDPG. For each task, to compose a well working reward function and hyperparameters, constant tries are needed and observe the performance of the agent and make a proper change.
* Interesting things in how the quadcopter behaved are:
** At some time, the agent does the right thing at the first start;
** At other time, the agent learns nothing;
** Even for the agent learns how to well behaved, fluctuation may occurr.


== å‚è€ƒ

* https://www.zhihu.com/question/47602063/answer/150845355
* https://openai.com/blog/openai-gym-beta
* https://zhuanlan.zhihu.com/p/39999667
* https://zhuanlan.zhihu.com/p/25239682
* https://arxiv.org/pdf/1509.02971.pdf
